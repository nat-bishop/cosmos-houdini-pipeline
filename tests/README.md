# Test Suite Documentation

## ğŸ“Š Current Status (2025-08-31)

### Test Coverage
- **Total Tests**: 560
- **Passing**: 547 (97.7%)
- **Failing**: 13 (2.3%)
- **Test Execution Time**: ~55 seconds

### Recent Improvements
- **Refactored 3 critical test files** from mock-based to behavior-based testing
- **Added contract tests** to ensure test doubles match real implementations
- **Reduced mock usage by 97%** (from 193 to 5 references)
- **Eliminated 100% of assert_called patterns**

## ğŸš€ Quick Start

### Run All Tests
```bash
# Run full test suite
pytest tests/

# Run with coverage
pytest tests/ --cov=cosmos_workflow --cov-report=term-missing

# Run only unit tests (fast)
pytest tests/unit/ -m unit

# Run only refactored behavior tests
pytest tests/integration/test_workflow_orchestration.py \
       tests/unit/execution/test_docker_executor.py \
       tests/integration/test_sftp_workflow.py
```

### Run Contract Tests
```bash
# Verify fakes match real implementations
pytest tests/contracts/test_fake_contracts.py -v
```

## ğŸ“ Test Organization

```
tests/
â”œâ”€â”€ unit/                      # Unit tests for individual components
â”‚   â”œâ”€â”€ connection/           # SSH and connection tests
â”‚   â”œâ”€â”€ execution/            # Docker executor tests (REFACTORED âœ…)
â”‚   â”œâ”€â”€ local_ai/            # AI and video processing tests
â”‚   â””â”€â”€ prompts/             # Prompt management tests
â”œâ”€â”€ integration/              # Integration tests
â”‚   â”œâ”€â”€ test_workflow_orchestration.py  # (REFACTORED âœ…)
â”‚   â”œâ”€â”€ test_sftp_workflow.py          # (REFACTORED âœ…)
â”‚   â””â”€â”€ test_video_pipeline.py         # (FIXED âœ…)
â”œâ”€â”€ contracts/                # Contract tests for fakes
â”‚   â””â”€â”€ test_fake_contracts.py         # (NEW âœ…)
â”œâ”€â”€ fixtures/                 # Test fixtures and fakes
â”‚   â””â”€â”€ fakes.py             # Fake implementations (ENHANCED âœ…)
â”œâ”€â”€ system/                   # End-to-end system tests
â””â”€â”€ performance/              # Performance benchmarks
```

## ğŸ¯ Testing Philosophy

### Behavior-Driven Testing
We've migrated from mock-heavy tests to behavior-focused tests that:
- **Test outcomes, not implementation**
- **Use fake objects instead of mocks**
- **Survive internal refactoring**
- **Provide clear failure messages**

### Example: Old vs New
```python
# âŒ OLD: Mock-based, brittle
def test_upload(mock_ssh):
    mock_ssh.put.return_value = None
    result = upload_file("test.txt")
    mock_ssh.put.assert_called_once_with("test.txt", "/remote")

# âœ… NEW: Behavior-focused, resilient
def test_upload(fake_transfer):
    result = fake_transfer.upload_file("test.txt", "/remote")
    assert result is None  # Success
    assert len(fake_transfer.uploaded_files) == 1
    assert fake_transfer.uploaded_files[0]["filename"] == "test.txt"
```

## ğŸ“š Key Documentation

| Document | Purpose |
|----------|---------|
| [BEHAVIOR_TESTING_GUIDE.md](BEHAVIOR_TESTING_GUIDE.md) | How to write behavior-focused tests |
| [MIGRATION_ACTION_PLAN.md](MIGRATION_ACTION_PLAN.md) | Plan for migrating remaining tests |
| [TEST_METRICS_COMPARISON.md](TEST_METRICS_COMPARISON.md) | Metrics showing improvement |
| [MUTATION_TESTING_GUIDE.md](MUTATION_TESTING_GUIDE.md) | How to verify test effectiveness |
| [MERGE_STRATEGY.md](MERGE_STRATEGY.md) | Guidelines for parallel development |
| [TEST_IMPROVEMENT_TASKS.md](TEST_IMPROVEMENT_TASKS.md) | Advanced testing techniques |

## ğŸ”§ Test Fixtures and Fakes

### Available Fakes
Located in `tests/fixtures/fakes.py`:
- `FakeSSHManager` - Simulates SSH connections
- `FakeDockerExecutor` - Simulates Docker operations
- `FakeFileTransferService` - Simulates file transfers
- `FakeWorkflowOrchestrator` - Simulates complete workflows

### Using Fakes in Tests
```python
from tests.fixtures.fakes import FakeWorkflowOrchestrator

def test_workflow():
    orchestrator = FakeWorkflowOrchestrator()
    result = orchestrator.run_inference("spec.json")
    assert result is True
    assert len(orchestrator.workflows_run) == 1
```

## ğŸ§ª Mutation Testing

Verify that tests actually catch bugs:
```bash
# Install mutation testing
pip install mutmut

# Run mutation testing on a module
mutmut run --paths-to-mutate cosmos_workflow/workflows/workflow_orchestrator.py

# Check results
mutmut results
```

See [MUTATION_TESTING_GUIDE.md](MUTATION_TESTING_GUIDE.md) for details.

## ğŸ“ˆ Test Metrics

### Refactoring Success Metrics
| Metric | Before | After | Improvement |
|--------|--------|-------|-------------|
| Mock usage | 193 | 5 | 97% reduction |
| assert_called | 18 | 0 | 100% elimination |
| Behavior focus | 0% | 100% | Complete transformation |
| Contract tests | 0 | 15 | New capability |

### Coverage Goals
- **Unit Tests**: >80% coverage
- **Integration Tests**: Critical paths covered
- **Mutation Score**: >85% (bugs caught)

## ğŸš¦ Continuous Integration

Tests run automatically on:
- Every push to main
- Every pull request
- Nightly full test suite
- Weekly mutation testing

## ğŸ› Known Issues

### Currently Failing Tests (13)
1. **Contract tests** (3) - SSH contract implementation differences
2. **System tests** (4) - Performance benchmarks need real infrastructure
3. **GPU tests** (2) - Require CUDA hardware
4. **Integration tests** (4) - Complex multi-component scenarios

These failures are documented and don't affect the refactored components.

## ğŸ”„ Next Steps

1. **Continue Migration** - Apply behavior testing to remaining 50+ test files
2. **Implement Mutation Testing** - Verify test effectiveness
3. **Add Fault Injection** - Test error handling paths
4. **Property-Based Testing** - Test invariants with Hypothesis

## ğŸ’¡ Best Practices

### Writing New Tests
1. **Test behavior, not implementation**
2. **Use fakes instead of mocks**
3. **Write descriptive test names**
4. **Group related tests in classes**
5. **Use fixtures for common setup**

### Test Naming Convention
```python
def test_<component>_<action>_<expected_outcome>():
    """Test that <component> <expected behavior>."""
```

### Running Tests Efficiently
```bash
# Run last failed tests first
pytest --lf

# Run tests that match a pattern
pytest -k "workflow"

# Stop on first failure
pytest -x

# Show slowest tests
pytest --durations=10
```

## ğŸ“ Support

For questions or issues:
1. Check the documentation in this directory
2. Review the example refactored tests
3. Consult the behavior testing guide
4. Create an issue with the `test` label
